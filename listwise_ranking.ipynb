{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JlLTP1Y-WHg"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "if-ujOZN-Par"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq9kCbELjzgJ"
      },
      "source": [
        "# Listwise ranking\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/listwise_ranking\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/listwise_ranking.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/listwise_ranking.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/listwise_ranking.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4BKWZyB_Hmf"
      },
      "source": [
        "In [the basic ranking tutorial](basic_ranking), we trained a model that can predict ratings for user/movie pairs. The model was trained to minimize the mean squared error of predicted ratings.\n",
        "\n",
        "However, optimizing the model's predictions on individual movies is not necessarily the best method for training ranking models. We do not need ranking models to predict scores with great accuracy. Instead, we care more about the ability of the model to generate an ordered list of items that matches the user's preference ordering.\n",
        "\n",
        "Instead of optimizing the model's predictions on individual query/item pairs, we can optimize the model's ranking of a list as a whole. This method is called _listwise ranking_.\n",
        "\n",
        "In this tutorial, we will use TensorFlow Recommenders to build listwise ranking models. To do so, we will make use of ranking losses and metrics provided by [TensorFlow Ranking](https://github.com/tensorflow/ranking), a TensorFlow package that focuses on [learning to rank](https://www.microsoft.com/en-us/research/publication/learning-to-rank-from-pairwise-approach-to-listwise-approach/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XS680n2n0rL"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "If TensorFlow Ranking is not available in your runtime environment, you can install it using `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr_BrcNMKji6"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "!pip install -q tensorflow-ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPQqa1uYKrw2"
      },
      "source": [
        "We can then import all the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ekaJkcuHsiY"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdTPCz136mvc"
      },
      "outputs": [],
      "source": [
        "import tensorflow_ranking as tfr\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNEB3VRs3bOS"
      },
      "source": [
        "We will continue to use the MovieLens 100K dataset. As before, we load the datasets and keep only the user id, movie title, and user rating features for this tutorial. We also do some houskeeping to prepare our vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-VF30hJn5-3"
      },
      "outputs": [],
      "source": [
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
        "\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": x[\"user_rating\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
        "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
        "    lambda x: x[\"user_id\"]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIBH-Axc7oqB"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "However, we cannot use the MovieLens dataset for list optimization directly. To perform listwise optimization, we need to have access to a list of movies each user has rated, but each example in the MovieLens 100K dataset contains only the rating of a single movie.\n",
        "\n",
        "To get around this we transform the dataset so that each example contains a user id and a list of movies rated by that user. Some movies in the list will be ranked higher than others; the goal of our model will be to make predictions that match this ordering.\n",
        "\n",
        "To do this, we use the `tfrs.examples.movielens.movielens_to_listwise` helper function. It takes the MovieLens 100K dataset and generates a dataset containing list examples as discussed above. The implementation details can be found in the [source code](https://github.com/tensorflow/recommenders/blob/main/tensorflow_recommenders/examples/movielens.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X99torl5z4Iu"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Split between train and tests sets, as before.\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "# We sample 50 lists for each user for the training data. For each list we\n",
        "# sample 5 movies from the movies the user rated.\n",
        "train = tfrs.examples.movielens.sample_listwise(\n",
        "    train,\n",
        "    num_list_per_user=50,\n",
        "    num_examples_per_list=5,\n",
        "    seed=42\n",
        ")\n",
        "test = tfrs.examples.movielens.sample_listwise(\n",
        "    test,\n",
        "    num_list_per_user=1,\n",
        "    num_examples_per_list=5,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zuAfGrgCBJP"
      },
      "source": [
        "We can inspect an example from the training data. The example includes a user id, a list of 10 movie ids, and their ratings by the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO52eZqOzOUV"
      },
      "outputs": [],
      "source": [
        "for example in train.take(1):\n",
        "  pprint.pprint(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM3uu5hgN4-v"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "We will train the same model with three different losses:\n",
        "\n",
        "- mean squared error,\n",
        "- pairwise hinge loss, and\n",
        "- a listwise ListMLE loss.\n",
        "\n",
        "These three losses correspond to pointwise, pairwise, and listwise optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXmqkuAShHO7"
      },
      "source": [
        "To evaluate the model we use [normalized discounted cumulative gain (NDCG)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG). NDCG measures a predicted ranking by taking a weighted sum of the actual rating of each candidate. The ratings of movies that are ranked lower by the model would be discounted more. As a result, a good model that ranks highly-rated movies on top would have a high NDCG result. Since this metric takes the ranked position of each candidate into account, it is a listwise metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1RTt67fhR52"
      },
      "outputs": [],
      "source": [
        "class RankingModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, loss):\n",
        "    super().__init__()\n",
        "    embedding_dimension = 32\n",
        "    self.loss_function = loss  # NDCGLoss2\n",
        "\n",
        "    # Compute embeddings for users.\n",
        "    self.user_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute embeddings for movies.\n",
        "    self.movie_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_titles),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 2, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute predictions.\n",
        "    self.score_model = tf.keras.Sequential([\n",
        "      # Learn multiple dense layers.\n",
        "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "      # Make rating predictions in the final layer.\n",
        "      tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    self.task = tfrs.tasks.Ranking(\n",
        "      loss=loss,\n",
        "      metrics=[\n",
        "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
        "        tf.keras.metrics.RootMeanSquaredError()\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  def call(self, features):\n",
        "    # We first convert the id features into embeddings.\n",
        "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
        "    user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
        "\n",
        "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
        "    # tensor.\n",
        "    movie_embeddings = self.movie_embeddings(features[\"movie_title\"])\n",
        "\n",
        "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
        "    # them into the ranking model. To do so, we need to reshape the user\n",
        "    # embeddings to match the shape of movie embeddings.\n",
        "    list_length = features[\"movie_title\"].shape[1]\n",
        "    user_embedding_repeated = tf.repeat(\n",
        "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
        "\n",
        "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
        "    # predictions.\n",
        "    concatenated_embeddings = tf.concat(\n",
        "        [user_embedding_repeated, movie_embeddings], 2)\n",
        "\n",
        "    return self.score_model(concatenated_embeddings)\n",
        "\n",
        "\n",
        "  # def compute_loss(self, features, training=False):\n",
        "  #   labels = features.pop(\"user_rating\")\n",
        "\n",
        "  #   scores = self(features)\n",
        "\n",
        "  #   return self.task(\n",
        "  #       labels=labels,\n",
        "  #       predictions=tf.squeeze(scores, axis=-1),\n",
        "  #   )\n",
        "  def compute_loss(self, features, training=False):\n",
        "      labels = features.pop(\"user_rating\")\n",
        "      scores = self(features)\n",
        "\n",
        "      # Compute loss using NDCGLoss2\n",
        "      return self.loss_function(labels, tf.squeeze(scores, axis=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcTElTWbOImt"
      },
      "source": [
        "## Training the models\n",
        "\n",
        "We can now train each of the three models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U530Yk-s-g9"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQNnD7DNTkYC"
      },
      "source": [
        "### Mean squared error model\n",
        "\n",
        "This model is very similar to the model in [the basic ranking tutorial](basic_ranking). We train the model to minimize the mean squared error between the actual ratings and predicted ratings. Therefore, this loss is computed individually for each movie and the training is pointwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0lq0Nq7_xTW"
      },
      "outputs": [],
      "source": [
        "mse_model = RankingModel(tf.keras.losses.MeanSquaredError())\n",
        "mse_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NBl543nRtIo"
      },
      "outputs": [],
      "source": [
        "mse_model.fit(cached_train, epochs=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DHphbdxUS7l"
      },
      "source": [
        "### Pairwise hinge loss model\n",
        "\n",
        "By minimizing the pairwise hinge loss, the model tries to maximize the difference between the model's predictions for a highly rated item and a low rated item: the bigger that difference is, the lower the model loss. However, once the difference is large enough, the loss becomes zero, stopping the model from further optimizing this particular pair and letting it focus on other pairs that are incorrectly ranked\n",
        "\n",
        "This loss is not computed for individual movies, but rather for pairs of movies. Hence the training using this loss is pairwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px_wZPxCOrBt"
      },
      "outputs": [],
      "source": [
        "hinge_model = RankingModel(tfr.keras.losses.PairwiseHingeLoss())\n",
        "hinge_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqbd9aDXO6mP"
      },
      "outputs": [],
      "source": [
        "hinge_model.fit(cached_train, epochs=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79_Y2cuUal-"
      },
      "source": [
        "### Listwise model\n",
        "\n",
        "The `ListMLE` loss from TensorFlow Ranking expresses list maximum likelihood estimation. To calculate the ListMLE loss, we first use the user ratings to generate an optimal ranking. We then calculate the likelihood of each candidate being out-ranked by any item below it in the optimal ranking using the predicted scores. The model tries to minimize such likelihood to ensure highly rated candidates are not out-ranked by low rated candidates. You can learn more about the details of ListMLE in section 2.2 of the paper [Position-aware ListMLE: A Sequential Learning Process](http://auai.org/uai2014/proceedings/individuals/164.pdf).\n",
        "\n",
        "Note that since the likelihood is computed with respect to a candidate and all candidates below it in the optimal ranking, the loss is not pairwise but listwise. Hence the training uses list optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IO8N2JQvASN"
      },
      "outputs": [],
      "source": [
        "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
        "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aA0lqGovDrw"
      },
      "outputs": [],
      "source": [
        "listwise_model.fit(cached_train, epochs=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5y0ucbFSEZi"
      },
      "source": [
        "## Comparing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfPgRvnXSJwm"
      },
      "outputs": [],
      "source": [
        "mse_model_result = mse_model.evaluate(cached_test, return_dict=True)\n",
        "print(\"NDCG of the MSE Model: {:.4f}\".format(mse_model_result[\"ndcg_metric\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwTaEJ6JPF9k"
      },
      "outputs": [],
      "source": [
        "hinge_model_result = hinge_model.evaluate(cached_test, return_dict=True)\n",
        "print(\"NDCG of the pairwise hinge loss model: {:.4f}\".format(hinge_model_result[\"ndcg_metric\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR8xQs6BSO0X"
      },
      "outputs": [],
      "source": [
        "listwise_model_result = listwise_model.evaluate(cached_test, return_dict=True)\n",
        "print(\"NDCG of the ListMLE model: {:.4f}\".format(listwise_model_result[\"ndcg_metric\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XeWqIq4O1xo"
      },
      "source": [
        "Of the three models, the model trained using ListMLE has the highest NDCG metric. This result shows how listwise optimization can be used to train ranking models and can potentially produce models that perform better than models optimized in a pointwise or pairwise fashion."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# class NDCGLoss2(tf.keras.losses.Loss):\n",
        "#     def __init__(self, sigma=1.0, reduction=tf.keras.losses.Reduction.AUTO, name=None):\n",
        "#         super().__init__(reduction=reduction, name=name)\n",
        "#         self.sigma = sigma\n",
        "\n",
        "#     def call(self, y_true, y_pred):\n",
        "#         # Ensure that y_true and y_pred are the same shape and dtype\n",
        "#         y_true = tf.cast(y_true, tf.float32)\n",
        "#         y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "#         # Compute pairwise differences in scores\n",
        "#         y_pred_diff = tf.expand_dims(y_pred, 2) - tf.expand_dims(y_pred, 1)\n",
        "\n",
        "#         # Mask for pairs where y_i > y_j\n",
        "#         mask = tf.greater(tf.expand_dims(y_true, 2), tf.expand_dims(y_true, 1))\n",
        "\n",
        "#         # Sigmoid function applied to score differences\n",
        "#         sigmoid_scores = tf.sigmoid(self.sigma * y_pred_diff)\n",
        "\n",
        "#         # Rank difference term\n",
        "#         num_items = tf.shape(y_true)[1]\n",
        "#         rank_diff = tf.abs(tf.range(num_items)[:, None] - tf.range(num_items)[None, :])\n",
        "#         rank_diff = tf.cast(rank_diff, tf.float32)\n",
        "#         epsilon = 1e-10  # A small constant to avoid division by zero\n",
        "#         log_rank_term = tf.abs(1 / (tf.math.log1p(rank_diff) + epsilon) - 1 / (tf.math.log1p(rank_diff + 1) + epsilon))\n",
        "#         # Expand log_rank_term to match the batch size\n",
        "#         log_rank_term = tf.expand_dims(log_rank_term, 0)\n",
        "#         log_rank_term = tf.repeat(log_rank_term, tf.shape(y_true)[0], axis=0)\n",
        "\n",
        "#         # Simplified Hard Assignment Distribution H(pi|s)\n",
        "#         predicted_order = tf.argsort(y_pred, axis=1, direction='DESCENDING')\n",
        "#         true_order = tf.argsort(y_true, axis=1, direction='DESCENDING')\n",
        "#         H_pi_s = tf.cast(tf.equal(predicted_order[:, :, None], true_order[:, None, :]), tf.float32)\n",
        "\n",
        "#         # Compute the inner sum\n",
        "#         inner_sum = H_pi_s * sigmoid_scores**log_rank_term\n",
        "\n",
        "#         # Apply the mask and compute the loss\n",
        "#         masked_inner_sum = tf.boolean_mask(inner_sum, mask)\n",
        "#         loss = -tf.reduce_sum(tf.math.log1p(masked_inner_sum), axis=-1)\n",
        "#         # tf.print(\"Loss value:\", loss)\n",
        "\n",
        "#         return loss"
      ],
      "metadata": {
        "id": "YgaweUBAcc5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NDCGLoss2(tf.keras.losses.Loss):\n",
        "    def __init__(self, sigma=1.0, reduction=tf.keras.losses.Reduction.AUTO, name=None):\n",
        "        super().__init__(reduction=reduction, name=name)\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Ensure that y_true and y_pred are the same shape and dtype\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "        # Compute pairwise differences in scores\n",
        "        y_pred_diff = tf.expand_dims(y_pred, 2) - tf.expand_dims(y_pred, 1)\n",
        "\n",
        "        # Mask for pairs where y_i > y_j\n",
        "        mask = tf.greater(tf.expand_dims(y_true, 2), tf.expand_dims(y_true, 1))\n",
        "\n",
        "        # Sigmoid function applied to score differences\n",
        "        sigmoid_scores = tf.sigmoid(self.sigma * y_pred_diff)\n",
        "\n",
        "        # Rank difference term\n",
        "        num_items = tf.shape(y_true)[1]\n",
        "        rank_diff = tf.abs(tf.range(num_items)[:, None] - tf.range(num_items)[None, :])\n",
        "        rank_diff = tf.cast(rank_diff, tf.float32)\n",
        "        epsilon = 1e-10  # A small constant to avoid division by zero\n",
        "        log_rank_term = tf.abs(1 / (tf.math.log1p(rank_diff) + epsilon) - 1 / (tf.math.log1p(rank_diff + 1) + epsilon))\n",
        "\n",
        "        # Expand log_rank_term to match the batch size\n",
        "        log_rank_term = tf.expand_dims(log_rank_term, 0)\n",
        "        log_rank_term = tf.repeat(log_rank_term, tf.shape(y_true)[0], axis=0)\n",
        "\n",
        "        # Simplified Hard Assignment Distribution H(pi|s)\n",
        "        predicted_order = tf.argsort(y_pred, axis=1, direction='DESCENDING')\n",
        "        true_order = tf.argsort(y_true, axis=1, direction='DESCENDING')\n",
        "        H_pi_s = tf.cast(tf.equal(predicted_order[:, :, None], true_order[:, None, :]), tf.float32)\n",
        "\n",
        "        # Compute the inner sum\n",
        "        inner_sum = H_pi_s * sigmoid_scores**log_rank_term\n",
        "\n",
        "        # Apply the mask and compute the loss\n",
        "        masked_inner_sum = tf.boolean_mask(inner_sum, mask)\n",
        "        loss = -tf.reduce_sum(tf.math.log1p(masked_inner_sum), axis=-1)\n",
        "\n",
        "        # Check for NaN or Inf in tensors and print if present\n",
        "        if tf.reduce_any(tf.math.is_nan(loss)) or tf.reduce_any(tf.math.is_inf(loss)):\n",
        "            tf.print(\"y_true:\", y_true)\n",
        "            tf.print(\"y_pred:\", y_pred)\n",
        "            tf.print(\"y_pred_diff:\", y_pred_diff)\n",
        "            tf.print(\"mask:\", mask)\n",
        "            tf.print(\"sigmoid_scores:\", sigmoid_scores)\n",
        "            tf.print(\"expanded log_rank_term:\", log_rank_term)\n",
        "            tf.print(\"H_pi_s:\", H_pi_s)\n",
        "            tf.print(\"inner_sum:\", inner_sum)\n",
        "            tf.print(\"Loss value:\", loss)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "4iuVATAShDqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customModel = RankingModel(NDCGLoss2())\n",
        "# customModel.compile(optimizer=tf.keras.optimizers.Adam(0.1))"
      ],
      "metadata": {
        "id": "76Crk67icd82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# customModel.fit(cached_train, epochs=10, verbose=True)\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.1)"
      ],
      "metadata": {
        "id": "yRWhD4zjcjk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(model, features, optimizer, clip_norm=1.0):\n",
        "    # Create a copy of features to avoid modifying the original argument\n",
        "    features_copy = {key: tf.identity(value) for key, value in features.items()}\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = model.compute_loss(features_copy)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
        "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "DbFkdWt9cobj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndcg_metric = tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\")"
      ],
      "metadata": {
        "id": "csLtpzISl7h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Context manager to suppress stdout\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout"
      ],
      "metadata": {
        "id": "EPT34NIEmVxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    # Reset the metric at the start of each epoch for training\n",
        "    ndcg_metric.reset_states()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_features in cached_train:\n",
        "        loss = train_step(customModel, batch_features, optimizer)\n",
        "        total_loss += loss\n",
        "\n",
        "        with suppress_stdout():\n",
        "            predictions = customModel.predict(batch_features)\n",
        "\n",
        "        predictions = tf.squeeze(predictions)\n",
        "        ndcg_metric.update_state(batch_features['user_rating'], predictions)\n",
        "\n",
        "    # Compute average loss over the epoch for training\n",
        "    average_loss = total_loss / len(cached_train)\n",
        "\n",
        "    # Get the training NDCG result\n",
        "    ndcg_train_result = ndcg_metric.result().numpy()\n",
        "\n",
        "    # Reset the metric for test evaluation\n",
        "    ndcg_metric.reset_states()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    for batch_features in cached_test:\n",
        "        with suppress_stdout():\n",
        "            predictions = customModel.predict(batch_features)\n",
        "\n",
        "        predictions = tf.squeeze(predictions)\n",
        "        ndcg_metric.update_state(batch_features['user_rating'], predictions)\n",
        "\n",
        "    # Get the test NDCG result\n",
        "    ndcg_test_result = ndcg_metric.result().numpy()\n",
        "\n",
        "    # Print epoch results for both training and test\n",
        "    print(f\"Epoch {epoch}: Average Loss: {average_loss.numpy()}, Training NDCG: {ndcg_train_result}, Test NDCG: {ndcg_test_result}\")\n"
      ],
      "metadata": {
        "id": "A8RYOyP8zZEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    # Reset the metric at the start of each epoch for training\n",
        "    ndcg_metric.reset_states()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_features in cached_train:\n",
        "        loss = train_step(customModel, batch_features, optimizer)\n",
        "        total_loss += loss\n",
        "\n",
        "        with suppress_stdout():\n",
        "            predictions = customModel.predict(batch_features)\n",
        "\n",
        "        predictions = tf.squeeze(predictions)\n",
        "        ndcg_metric.update_state(batch_features['user_rating'], predictions)\n",
        "\n",
        "    # Compute average loss over the epoch for training\n",
        "    average_loss = total_loss / len(cached_train)\n",
        "\n",
        "    # Get the training NDCG result\n",
        "    ndcg_train_result = ndcg_metric.result().numpy()\n",
        "\n",
        "    # Reset the metric for test evaluation\n",
        "    ndcg_metric.reset_states()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    for batch_features in cached_test:\n",
        "        with suppress_stdout():\n",
        "            predictions = customModel.predict(batch_features)\n",
        "\n",
        "        predictions = tf.squeeze(predictions)\n",
        "        ndcg_metric.update_state(batch_features['user_rating'], predictions)\n",
        "\n",
        "    # Get the test NDCG result\n",
        "    ndcg_test_result = ndcg_metric.result().numpy()\n",
        "\n",
        "    # Print epoch results for both training and test\n",
        "    print(f\"Epoch {epoch}: Average Loss: {average_loss.numpy()}, Training NDCG: {ndcg_train_result}, Test NDCG: {ndcg_test_result}\")\n"
      ],
      "metadata": {
        "id": "y6NTAX3ixb_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for epoch in range(5):\n",
        "#     # Reset the metric at the start of each epoch\n",
        "#     ndcg_metric.reset_states()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch_features in cached_train:\n",
        "#         loss = train_step(customModel, batch_features, optimizer)\n",
        "#         total_loss += loss\n",
        "\n",
        "#         # Suppress the output of predict\n",
        "#         with suppress_stdout():\n",
        "#             predictions = customModel.predict(batch_features)\n",
        "\n",
        "#         predictions = tf.squeeze(predictions)\n",
        "#         ndcg_metric.update_state(batch_features['user_rating'], predictions)\n",
        "\n",
        "#     # Compute average loss over the epoch\n",
        "#     average_loss = total_loss / len(cached_train)\n",
        "\n",
        "#     # Get the result from the metric\n",
        "#     ndcg_result = ndcg_metric.result().numpy()\n",
        "\n",
        "#     # Print epoch results\n",
        "#     print(f\"Epoch {epoch}: Average Loss: {average_loss.numpy()}, NDCG: {ndcg_result}\")"
      ],
      "metadata": {
        "id": "AodiP6WzkccC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# customModel_result = customModel.evaluate(cached_test, return_dict=True)\n",
        "# print(\"NDCG of the Custom model: {:.4f}\".format(customModel_result[\"ndcg_metric\"]))"
      ],
      "metadata": {
        "id": "EncRco81kd2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow_ranking as tfr\n",
        "\n",
        "# # Step 1: Create an instance of the NDCG metric\n",
        "# ndcg_metric = tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\")\n",
        "\n",
        "# # Step 2: Generate predictions and update the metric\n",
        "# for batch_features in cached_test:\n",
        "#     # Obtain predictions\n",
        "#     predictions = customModel.predict(batch_features)\n",
        "#     predictions = tf.squeeze(predictions)  # Ensure the correct shape\n",
        "\n",
        "#     # Update the NDCG metric\n",
        "#     ndcg_metric.update_state(batch_features['user_rating'], predictions)\n",
        "\n",
        "# # Step 3: Compute the NDCG score\n",
        "# ndcg_score = ndcg_metric.result().numpy()\n",
        "\n",
        "# print(\"NDCG of the Custom model on test set: {:.4f}\".format(ndcg_score))\n"
      ],
      "metadata": {
        "id": "xXMJjQ77wUjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHpZb7exwnlF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "listwise_ranking.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}