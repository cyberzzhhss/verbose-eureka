{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90637b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "# import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d41a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626c1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d3eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.datasets import msrank_10k\n",
    "train_df, test_df = msrank_10k()\n",
    "X_train = train_df.drop([0, 1], axis=1).values\n",
    "y_train = train_df[0].values\n",
    "queries_train = train_df[1].values\n",
    "\n",
    "X_test = test_df.drop([0, 1], axis=1).values\n",
    "y_test = test_df[0].values\n",
    "queries_test = test_df[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6898fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "955dfcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_train_scaled_shape': (10000, 136),\n",
       " 'X_test_scaled_shape': (10000, 136),\n",
       " 'y_train_scaled_shape': (10000,),\n",
       " 'y_test_scaled_shape': (10000,)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale targets\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "{\n",
    "    \"X_train_scaled_shape\": X_train_scaled.shape,\n",
    "    \"X_test_scaled_shape\": X_test_scaled.shape,\n",
    "    \"y_train_scaled_shape\": y_train_scaled.shape,\n",
    "    \"y_test_scaled_shape\": y_test_scaled.shape\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d89811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSRankModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        super().__init__()\n",
    "        self.score_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"tanh\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"tanh\"),\n",
    "            # Ensure the final layer has an output shape of [batch_size, 1]\n",
    "            tf.keras.layers.Dense(1, activation=None)\n",
    "        ])\n",
    "        \n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                # Specify NDCG with topn=10\n",
    "                tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\", topn=10),\n",
    "                tf.keras.metrics.RootMeanSquaredError()\n",
    "            ]\n",
    "        )\n",
    "#         self.task = tfrs.tasks.Ranking(\n",
    "#             loss=loss,\n",
    "#             metrics=[\n",
    "#                 tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "#                 tf.keras.metrics.RootMeanSquaredError()\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "    def call(self, features):\n",
    "        # features is a [batch_size, num_features] tensor.\n",
    "        # Ensure that the output is a 2D tensor with shape [batch_size, 1]\n",
    "        return self.score_model(features)\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        features, labels = data\n",
    "        scores = self(features)\n",
    "\n",
    "        # The labels might also need to be reshaped to [batch_size, 1]\n",
    "        labels = tf.reshape(labels, (-1, 1))\n",
    "\n",
    "        return self.task(\n",
    "            labels=labels,\n",
    "            predictions=scores,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9810cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# Optionally, you can add batch size and caching for efficiency\n",
    "train_dataset = train_dataset.batch(2048).cache()\n",
    "\n",
    "# Convert testing data to TensorFlow dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "# Optionally, you can add batch size for efficiency\n",
    "test_dataset = test_dataset.batch(2048).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66ff361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of training with mean squared error\n",
    "mse_model = MSRankModel(tf.keras.losses.MeanSquaredError())\n",
    "mse_model.compile(optimizer=tf.keras.optimizers.legacy.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012307aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - ndcg_metric: 0.4245 - root_mean_squared_error: 0.7995 - loss: 0.6368 - regularization_loss: 0.0000e+00 - total_loss: 0.6368\n",
      "NDCG of the MSE Model: 0.4245\n"
     ]
    }
   ],
   "source": [
    "mse_model.fit(train_dataset, epochs=20, verbose=False)\n",
    "mse_model_result = mse_model.evaluate(test_dataset, return_dict=True)\n",
    "print(\"NDCG of the MSE Model: {:.4f}\".format(mse_model_result[\"ndcg_metric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a45e581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - ndcg_metric: 0.4245 - root_mean_squared_error: 1.2235 - loss: 0.0000e+00 - regularization_loss: 0.0000e+00 - total_loss: 0.0000e+00\n",
      "NDCG of the Pairwise Hinge Loss Model: 0.4245\n"
     ]
    }
   ],
   "source": [
    "# Pairwise Hinge Loss Model\n",
    "hinge_model = MSRankModel(tfr.keras.losses.PairwiseHingeLoss())\n",
    "hinge_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(0.05))\n",
    "\n",
    "# Train the model\n",
    "hinge_model.fit(train_dataset, epochs=20, verbose=False)\n",
    "\n",
    "# Evaluate the model\n",
    "hinge_model_result = hinge_model.evaluate(test_dataset, return_dict=True)\n",
    "print(\"NDCG of the Pairwise Hinge Loss Model: {:.4f}\".format(hinge_model_result[\"ndcg_metric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acec26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - ndcg_metric: 0.4245 - root_mean_squared_error: 1.0585 - loss: 0.0000e+00 - regularization_loss: 0.0000e+00 - total_loss: 0.0000e+00\n",
      "NDCG of the ListMLE Loss Model: 0.4245\n"
     ]
    }
   ],
   "source": [
    "# ListMLE Loss Model\n",
    "listwise_model = MSRankModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.legacy.Adagrad(0.1))\n",
    "\n",
    "# Train the model\n",
    "listwise_model.fit(train_dataset, epochs=5, verbose=False)\n",
    "\n",
    "# Evaluate the model\n",
    "listwise_model_result = listwise_model.evaluate(test_dataset, return_dict=True)\n",
    "print(\"NDCG of the ListMLE Loss Model: {:.4f}\".format(listwise_model_result[\"ndcg_metric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b4d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84552dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53115b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((10000, 136), (10000,), (10000,)), ((10000, 136), (10000,), (10000,)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_complex_random_data(num_samples=10000, num_features=136, num_queries=1000):\n",
    "    \"\"\"\n",
    "    Generate a complex synthetic dataset with random features and labels.\n",
    "\n",
    "    Args:\n",
    "    - num_samples (int): Number of samples to generate.\n",
    "    - num_features (int): Number of features in each sample.\n",
    "    - num_queries (int): Number of unique queries.\n",
    "\n",
    "    Returns:\n",
    "    Tuple of Numpy arrays: (features, labels, query_ids)\n",
    "    \"\"\"\n",
    "    # Random features\n",
    "    X = np.random.randn(num_samples, num_features).astype(np.float32)\n",
    "\n",
    "    # Random labels, but more complex than a simple uniform distribution\n",
    "    # Using a combination of different distributions to add complexity\n",
    "    y = np.random.choice([np.random.normal(), np.random.exponential(), np.random.uniform()], size=num_samples)\n",
    "    y = (y - y.min()) / (y.max() - y.min()) * 4  # Normalize and scale labels\n",
    "\n",
    "    # Generate query IDs to group features into queries\n",
    "    query_ids = np.random.randint(0, num_queries, size=num_samples).astype(np.int32)\n",
    "\n",
    "    return X, y, query_ids\n",
    "\n",
    "# Generate the complex random dataset\n",
    "X_train_complex, y_train_complex, queries_train_complex = generate_complex_random_data()\n",
    "X_test_complex, y_test_complex, queries_test_complex = generate_complex_random_data()\n",
    "\n",
    "# Output shapes as a basic validation of the data generation\n",
    "(X_train_complex.shape, y_train_complex.shape, queries_train_complex.shape), (X_test_complex.shape, y_test_complex.shape, queries_test_complex.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff6f21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_involved_data(num_queries=1000, num_samples_per_query=10, num_features=136):\n",
    "    X = []\n",
    "    y = []\n",
    "    query_ids = []\n",
    "\n",
    "    for q_id in range(num_queries):\n",
    "        num_samples = np.random.randint(5, num_samples_per_query + 1)  # Random number of samples per query\n",
    "\n",
    "        # Generate features for each sample in the query\n",
    "        X_query = np.random.randn(num_samples, num_features).astype(np.float32)\n",
    "\n",
    "        # Generate labels for each sample in the query\n",
    "        # Example: labels could be dependent on some specific features\n",
    "        y_query = np.sum(X_query[:, :5], axis=1) + np.random.randn(num_samples)\n",
    "        y_query = (y_query - y_query.min()) / (y_query.max() - y_query.min()) * 4\n",
    "\n",
    "        # Append to the main list\n",
    "        X.append(X_query)\n",
    "        y.append(y_query)\n",
    "        query_ids.append(np.full(num_samples, q_id, dtype=np.int32))\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    return np.vstack(X), np.hstack(y), np.hstack(query_ids)\n",
    "\n",
    "# Generate the query-involved dataset\n",
    "X_train_query, y_train_query, queries_train_query = generate_query_involved_data()\n",
    "X_test_query, y_test_query, queries_test_query = generate_query_involved_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a0c4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MSRankModel class\n",
    "class MSRankModel(tfrs.Model):\n",
    "    def __init__(self, loss):\n",
    "        super().__init__()\n",
    "        self.score_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"tanh\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"tanh\"),\n",
    "            tf.keras.layers.Dense(1, activation=None)\n",
    "        ])\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=loss,\n",
    "            metrics=[tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\", topn=100),\n",
    "                     tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        return self.score_model(features)\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        features, labels = data\n",
    "        scores = self(features)\n",
    "        labels = tf.reshape(labels, (-1, 1))\n",
    "        return self.task(labels=labels, predictions=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4baaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with different loss functions\n",
    "loss_functions = [tf.keras.losses.MeanSquaredError(), tfr.keras.losses.PairwiseHingeLoss(), tfr.keras.losses.ListMLELoss()]\n",
    "loss_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b1e4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic data to TensorFlow dataset\n",
    "train_dataset_syn = tf.data.Dataset.from_tensor_slices((X_train_query, y_train_query)).batch(2048).cache()\n",
    "test_dataset_syn = tf.data.Dataset.from_tensor_slices((X_test_query, y_test_query)).batch(2048).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "068f4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - ndcg_metric: 0.8674 - root_mean_squared_error: 1.8310 - loss: 3.3197 - regularization_loss: 0.0000e+00 - total_loss: 3.3197\n",
      "4/4 [==============================] - 0s 2ms/step - ndcg_metric: 0.8674 - root_mean_squared_error: 2.5157 - loss: 0.0000e+00 - regularization_loss: 0.0000e+00 - total_loss: 0.0000e+00\n",
      "4/4 [==============================] - 0s 2ms/step - ndcg_metric: 0.8674 - root_mean_squared_error: 2.5408 - loss: 0.0000e+00 - regularization_loss: 0.0000e+00 - total_loss: 0.0000e+00\n",
      "{'<keras.src.losses.MeanSquaredError object at 0x2ef7dcb50>': 0.8674443364143372, '<tensorflow_ranking.python.keras.losses.PairwiseHingeLoss object at 0x2f1425e90>': 0.8674443364143372, '<tensorflow_ranking.python.keras.losses.ListMLELoss object at 0x2efd22810>': 0.8674443364143372}\n"
     ]
    }
   ],
   "source": [
    "# ... [previous code for data generation and model definition]\n",
    "\n",
    "# Use the legacy optimizer for compatibility with M1/M2 Macs\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(0.1)\n",
    "\n",
    "for loss_function in loss_functions:\n",
    "    model = MSRankModel(loss_function)\n",
    "    model.compile(optimizer=optimizer)\n",
    "    model.fit(train_dataset_syn, epochs=5, verbose=False)  # Set verbose to True to see progress\n",
    "    results = model.evaluate(test_dataset_syn, return_dict=True)\n",
    "    loss_results[str(loss_function)] = results[\"ndcg_metric\"]\n",
    "\n",
    "print(loss_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b3df88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "183b89ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - ndcg_metric: 0.8676 - root_mean_squared_error: 2.0666 - loss: 4.2987 - regularization_loss: 0.0000e+00 - total_loss: 4.2987\n",
      "4/4 [==============================] - 0s 2ms/step - ndcg_metric: 0.8676 - root_mean_squared_error: 2.5194 - loss: 0.0000e+00 - regularization_loss: 0.0000e+00 - total_loss: 0.0000e+00\n",
      "4/4 [==============================] - 0s 2ms/step - ndcg_metric: 0.8676 - root_mean_squared_error: 2.5212 - loss: 0.0000e+00 - regularization_loss: 0.0000e+00 - total_loss: 0.0000e+00\n",
      "{'<keras.src.losses.MeanSquaredError object at 0x2f2706990>': 0.867584764957428, '<tensorflow_ranking.python.keras.losses.PairwiseHingeLoss object at 0x2f3b8da10>': 0.867584764957428, '<tensorflow_ranking.python.keras.losses.ListMLELoss object at 0x2f2cb5890>': 0.867584764957428}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_complex_random_data(num_samples=10000, num_features=136, num_queries=1000):\n",
    "    \"\"\"\n",
    "    Generate a complex synthetic dataset with random features and labels.\n",
    "\n",
    "    Args:\n",
    "    - num_samples (int): Number of samples to generate.\n",
    "    - num_features (int): Number of features in each sample.\n",
    "    - num_queries (int): Number of unique queries.\n",
    "\n",
    "    Returns:\n",
    "    Tuple of Numpy arrays: (features, labels, query_ids)\n",
    "    \"\"\"\n",
    "    # Random features\n",
    "    X = np.random.randn(num_samples, num_features).astype(np.float32)\n",
    "\n",
    "    # Random labels, but more complex than a simple uniform distribution\n",
    "    # Using a combination of different distributions to add complexity\n",
    "    y = np.random.choice([np.random.normal(), np.random.exponential(), np.random.uniform()], size=num_samples)\n",
    "    y = (y - y.min()) / (y.max() - y.min()) * 4  # Normalize and scale labels\n",
    "\n",
    "    # Generate query IDs to group features into queries\n",
    "    query_ids = np.random.randint(0, num_queries, size=num_samples).astype(np.int32)\n",
    "\n",
    "    return X, y, query_ids\n",
    "\n",
    "# Generate the complex random dataset\n",
    "X_train_complex, y_train_complex, queries_train_complex = generate_complex_random_data()\n",
    "X_test_complex, y_test_complex, queries_test_complex = generate_complex_random_data()\n",
    "\n",
    "# Output shapes as a basic validation of the data generation\n",
    "(X_train_complex.shape, y_train_complex.shape, queries_train_complex.shape), (X_test_complex.shape, y_test_complex.shape, queries_test_complex.shape)\n",
    "\n",
    "\n",
    "\n",
    "def generate_query_involved_data(num_queries=1000, num_samples_per_query=10, num_features=136):\n",
    "    X = []\n",
    "    y = []\n",
    "    query_ids = []\n",
    "\n",
    "    for q_id in range(num_queries):\n",
    "        num_samples = np.random.randint(5, num_samples_per_query + 1)  # Random number of samples per query\n",
    "\n",
    "        # Generate features for each sample in the query\n",
    "        X_query = np.random.randn(num_samples, num_features).astype(np.float32)\n",
    "\n",
    "        # Generate labels for each sample in the query\n",
    "        # Example: labels could be dependent on some specific features\n",
    "        y_query = np.sum(X_query[:, :5], axis=1) + np.random.randn(num_samples)\n",
    "        y_query = (y_query - y_query.min()) / (y_query.max() - y_query.min()) * 4\n",
    "\n",
    "        # Append to the main list\n",
    "        X.append(X_query)\n",
    "        y.append(y_query)\n",
    "        query_ids.append(np.full(num_samples, q_id, dtype=np.int32))\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    return np.vstack(X), np.hstack(y), np.hstack(query_ids)\n",
    "\n",
    "# Generate the query-involved dataset\n",
    "X_train_query, y_train_query, queries_train_query = generate_query_involved_data()\n",
    "X_test_query, y_test_query, queries_test_query = generate_query_involved_data()\n",
    "\n",
    "\n",
    "# Define the MSRankModel class\n",
    "class MSRankModel(tfrs.Model):\n",
    "    def __init__(self, loss):\n",
    "        super().__init__()\n",
    "        self.score_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"tanh\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"tanh\"),\n",
    "            tf.keras.layers.Dense(1, activation=None)\n",
    "        ])\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=loss,\n",
    "            metrics=[tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\", topn=100),\n",
    "                     tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        return self.score_model(features)\n",
    "\n",
    "    def compute_loss(self, data, training=False):\n",
    "        features, labels = data\n",
    "        scores = self(features)\n",
    "        labels = tf.reshape(labels, (-1, 1))\n",
    "        return self.task(labels=labels, predictions=scores)\n",
    "\n",
    "\n",
    "# Test the model with different loss functions\n",
    "loss_functions = [tf.keras.losses.MeanSquaredError(), tfr.keras.losses.PairwiseHingeLoss(), tfr.keras.losses.ListMLELoss()]\n",
    "loss_results = {}\n",
    "\n",
    "# Convert synthetic data to TensorFlow dataset\n",
    "train_dataset_syn = tf.data.Dataset.from_tensor_slices((X_train_query, y_train_query)).batch(2048).cache()\n",
    "test_dataset_syn = tf.data.Dataset.from_tensor_slices((X_test_query, y_test_query)).batch(2048).cache()\n",
    "\n",
    "\n",
    "# ... [previous code for data generation and model definition]\n",
    "\n",
    "# Use the legacy optimizer for compatibility with M1/M2 Macs\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(0.1)\n",
    "\n",
    "for loss_function in loss_functions:\n",
    "    model = MSRankModel(loss_function)\n",
    "    model.compile(optimizer=optimizer)\n",
    "    model.fit(train_dataset_syn, epochs=5, verbose=False)  # Set verbose to True to see progress\n",
    "    results = model.evaluate(test_dataset_syn, return_dict=True)\n",
    "    loss_results[str(loss_function)] = results[\"ndcg_metric\"]\n",
    "\n",
    "print(loss_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db3a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209be8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44282f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509c2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8107b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5aac80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d22488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9cbd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1b312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109b6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbfe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bd09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b296e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "# import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92e26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = [tf.keras.losses.MeanSquaredError(), tfr.keras.losses.PairwiseHingeLoss(), tfr.keras.losses.ListMLELoss()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a5bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_related_data(num_queries=1000, num_samples_per_query=10, num_features=136):\n",
    "    X = []\n",
    "    y = []\n",
    "    query_ids = []\n",
    "\n",
    "    for q_id in range(num_queries):\n",
    "        num_samples = np.random.randint(5, num_samples_per_query + 1)\n",
    "\n",
    "        # Generate features for each sample in the query\n",
    "        X_query = np.random.randn(num_samples, num_features).astype(np.float32)\n",
    "\n",
    "        # Generate labels for each sample in the query\n",
    "        # Example: labels could be more influenced by certain features\n",
    "        y_query = np.sum(X_query[:, :5], axis=1) + np.random.randint(num_samples)\n",
    "        y_query = np.argsort(np.argsort(-y_query))  # Rank-based labels\n",
    "        y_query = tf.cast(y_query, dtype=tf.float32)\n",
    "\n",
    "        X.append(X_query)\n",
    "        y.append(y_query)\n",
    "        query_ids.append(np.full(num_samples, q_id, dtype=np.float32))\n",
    "\n",
    "    return np.vstack(X), np.hstack(y), np.hstack(query_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c818356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queries = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfc62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, queries_train = generate_query_related_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037ac5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, queries_test = generate_query_related_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497effc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7509, 136)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b3605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAwareRankModel(tfrs.Model):\n",
    "    def __init__(self, loss, num_queries):\n",
    "        super().__init__()\n",
    "        self.query_embedding = tf.keras.layers.Embedding(input_dim=num_queries, output_dim=64)\n",
    "        self.score_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"tanh\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"tanh\"),\n",
    "            tf.keras.layers.Dense(1, activation=None)\n",
    "        ])\n",
    "        self.task = tfrs.tasks.Ranking(loss=loss, metrics=[tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\")])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        features = inputs[\"features\"]\n",
    "        query_id = inputs[\"query_id\"]\n",
    "        query_embedding = self.query_embedding(query_id)\n",
    "        combined_features = tf.concat([features, query_embedding], axis=1)\n",
    "        return self.score_model(combined_features)\n",
    "\n",
    "\n",
    "    \n",
    "    def compute_loss(self, data, training=False):\n",
    "        data_dict, labels = data\n",
    "        features = data_dict[\"features\"]\n",
    "        query_id = data_dict[\"query_id\"]\n",
    "        scores = self(data_dict, training)\n",
    "        labels = tf.reshape(labels, (-1, 1))\n",
    "        return self.task(labels=labels, predictions=scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16b474d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'features': X_train, 'query_id': queries_train}, y_train)).batch(2048)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({'features': X_test, 'query_id': queries_test}, y_test)).batch(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c123216",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04cb0ebe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 2ms/step - ndcg_metric: 0.8670 - loss: 18.1650 - regularization_loss: 0.0000e+00 - total_loss: 18.1650\n",
      "4/4 [==============================] - 1s 2ms/step - ndcg_metric: 0.8670 - loss: 0.0000e+00 - regularization_loss: 0.0000e+00 - total_loss: 0.0000e+00\n",
      "4/4 [==============================] - 0s 2ms/step - ndcg_metric: 0.8670 - loss: 0.0000e+00 - regularization_loss: 0.0000e+00 - total_loss: 0.0000e+00\n",
      "{'<keras.src.losses.MeanSquaredError object at 0x2acfd6910>': 0.866950511932373, '<tensorflow_ranking.python.keras.losses.PairwiseHingeLoss object at 0x117a778d0>': 0.866950511932373, '<tensorflow_ranking.python.keras.losses.ListMLELoss object at 0x2ae4f2c10>': 0.866950511932373}\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(0.1)\n",
    "\n",
    "for loss_function in loss_functions:\n",
    "    model = QueryAwareRankModel(loss_function, num_queries)\n",
    "    model.compile(optimizer=optimizer)\n",
    "#     model.fit(train_dataset, epochs=5, verbose=False)\n",
    "    results = model.evaluate(test_dataset, return_dict=True)\n",
    "    loss_results[str(loss_function)] = results[\"ndcg_metric\"]\n",
    "\n",
    "print(loss_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee6cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
